{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"id\":\"9ku0h8OsoHJz\"}\n# <p align=\"center\"><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"260\" height=\"110\" /></p>\n\n# %% [markdown] {\"id\":\"-7Q1C_q31Ejc\"}\n# <center><img src=  \"https://images-na.ssl-images-amazon.com/images/I/71qrXkl0QJL._AC_SY355_.jpg\">\n# \n# \n\n# %% [markdown] {\"id\":\"DYXpnBWS1Ejd\"}\n# <center><a href=\"mailto:pati.simayan@gmail.com\" target=\"_blank\" rel=\"noopener\">Simayan Pati</a>\n\n# %% [markdown] {\"id\":\"GvPYamSK1Ejd\"}\n# This notebook is submitted for the fulfilment of **GCDAI - Term III**  curriculum [Nov 2020 batch] from **INSAID**. \n# \n#   - An **exploratory data analysis & data visualisation along with implementation of machine learning** been used covering the following topics\n#   \n#     - Python basics\n#     - Numerical computing with Numpy array operations \n#     - Analyzing tabular data with Pandas\n#     - Data visualization with Matplotlib and Seaborn \n#     - Fundamental of Machine Learning for prediction and classification using sklearn, SciPy & Skit-Learn\n#   - It contains a hypothetical case study for **Hormel Foods,USA** in context to under the pricing phemonon for Hass Avocados using the [available dataset](https://raw.githubusercontent.com/insaid2018/Term-2/master/Projects/avocado.csv).\n#   - This data was downloaded from the Hass Avocado Board website in May of 2018 & compiled into a single CSV. Here's [how the Hass Avocado Board describes the data on their website](https://hassavocadoboard.com/):\n# \n# > <font color='DarkMagenta'>*The table below represents weekly 2018 retail scan data for National retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. Starting in 2013, the table below reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLU’s) in the table are only for Hass avocados. Other varieties of avocados (e.g. greenskins) are not included in this table.*\n# \n#   - Many thanks to the Hass Avocado Board for sharing this [data](http://www.hassavocadoboard.com/retail/volume-and-price-data)\n\n# %% [markdown]\n# > The **Hass avocado** is a cultivar of avocado with dark green–colored, bumpy skin. It was first grown and sold by Southern California mail carrier and amateur horticulturist Rudolph Hass, who also gave it his name. The Hass avocado is a large-sized fruit weighing 200 to 300 grams. When ripe, the skin becomes a dark purplish-black and yields to gentle pressure. When ready to serve, it becomes white-green in the middle part of the inner fruit. Owing to its taste, size, shelf-life, high growing yield and in some areas, year-round harvesting, the Hass cultivar is the most commercially popular avocado worldwide. In the United States it accounts for more than 80% of the avocado crop, 95% of the California crop and is the most widely grown avocado in New Zealand.\n\n# %% [markdown] {\"id\":\"mA6VC1r8DH8g\"}\n# <a name = Section0></a>\n# ---\n# # **Table of Contents**\n# ---\n# \n# 1. [**Introduction**](#Section1)<br>\n# 2. [**Problem Statement**](#Section2)<br>\n# 3. [**Installing & Importing Libraries**](#Section3)<br>\n# 4. [**Data Acquisition & Description**](#Section4)<br>\n# 5. [**Data Pre-Processing**](#Section5)<br>\n# 6. [**Exploratory Data Analysis**](#Section6)<br>\n# 9. [**Summarization**](#Section9)</br>\n#   9.1 [**Conclusion**](#Section91)</br>\n#   9.2 [**Actionable Insights**](#Section91)</br>\n\n# %% [markdown] {\"id\":\"y7ahy5zG1Eje\"}\n# ---\n# <a name = Section1></a>\n# # **1. Introduction**                            \n# ---\n# [Go back to Index](#Section0)<br>   \n# <font color= 'OliveDrab'>**In which cities can millennials have their avocado toast AND buy a home?** It is a well known fact that Millenials LOVE Avocado Toast. It's also a well known fact that all Millennials live in their parents' basements.\n# Clearly, they aren't buying home because they are buying too much Avocado Toast! But maybe there's hope… if a Millennial could find a city with cheap avocados, they could live out the Millennial American Dream.<font>\n# \n# * Avocados are the darling of the produce section. They’re the go-to ingredient for guacamole dips at parties. And they're also turning up in everything from salads and wraps to smoothies and even brownies\n# \n# * Given the rise of Avocadopocalypse in 2017 **Hormel Foods Corporation**, makers of the **WHOLLY GUACAMOLE®** brand, America's #1 selling refrigerated guacamole is interested to understand the volatility & price dynamics basis available data.\n# \n\n# %% [markdown] {\"id\":\"SUOuq1Y22psu\"}\n#     WHOLLY® GUACAMOLE Classic Guacamole: \n#     Hass Avocados, Distilled Vinegar, Contains 2% Or Less of Water, Jalapeño Peppers, Salt, Dehydrated Onion, Granulated Garlic.\n\n# %% [markdown] {\"id\":\"Iq1uAduF1Ejf\"}\n# ---\n# <a name = Section2></a>\n# # **2. Problem Statement**\n# ---\n# [Go back to Index](#Section0)<br>   \n# \n# • Hormel Foods Corps avocados are sourced from over 1000 growers owning over 65,000 acres across California, Mexico, Chile, and Peru.\n# \n# • With generations of experience growing, packing, and shipping avocados, they have a deep understanding of the avocado industry.\n# \n# • Their aim is to source quality fruit that’s sustainably grown and handled in the most efficient, shortest supply route possible.\n# \n# • They want to increase their supply throughout the United States and need to make sure that they are selling their products at the best possible price.\n# \n# • Avocado prices have rocketed in recent years by up to 129%, with the average national price in the US of a single Hass avocado reaching $2.10 in 2019, almost doubling in just one year.\n# \n# • Due to this uncertainty in the prices, the company is not able to sell their produce at the optimal price.\n# \n# • **Task is to predict the optimal price of the avocado using the previous sales data of avocado according to different regions**.\n# \n\n# %% [markdown] {\"id\":\"MzDL9wBh1Ejh\"}\n# **Steps for our Forecasting Project:**\n# 1. Determine what is the problem: In this case we want to have accurate forecast of Avocado prices.\n# 2. Gathering Information: Understand what was the process that was used to gather the information and if the information is sufficient to have effective predictive models.\n# 3. Implementing Exploratory Analysis: Determine if there are any sort of patterns in our data before going into building the models.\n# 4. Choosing predictive models: This is the phase where we decide which model is the most appropriate to make our forecasting most effective.\n# 5. Testing our model: Analyze if our model is effective enough to make effective predictions.\n# \n# \n# \n\n# %% [markdown] {\"id\":\"SrpROjt2G_Jx\"}\n# ---\n# <a id = Section3></a>\n# # **3. Installing & Importing Libraries**\n# ---\n# [Go back to Index](#Section0)<br>   \n# - This section is emphasised on installing and importing the necessary libraries that will be required.\n\n# %% [code] {\"id\":\"RjFtVFlU1Ejh\"}\n#-------------------------------------------------------------------------------------------------------------------------------\nimport pandas as pd                                                 # Importing package pandas (For Panel Data Analysis)\n\n#-------------------------------------------------------------------------------------------------------------------------------\nimport numpy as np                                                  # Importing package numpys (For Numerical Python)\n#-------------------------------------------------------------------------------------------------------------------------------\nimport matplotlib.pyplot as plt                                     # Importing pyplot interface to use matplotlib\nimport seaborn as sns                                               # Importing seaborn library for interactive visualization\n%matplotlib inline\n#-------------------------------------------------------------------------------------------------------------------------------\nimport scipy as sp                                                  # Importing library for scientific calculations\n#-------------------------------------------------------------------------------------------------------------------------------\n\n# %% [code] {\"id\":\"ZVm5r-Zx1Eji\"}\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.mode.chained_assignment = None\n\n# %% [code] {\"id\":\"nDkVCDYz1Eji\"}\n# for data pipeline --------------------\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import*\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import make_pipeline\n\n# for prediction (machine learning models) ------------------------\n\nfrom sklearn.linear_model import*\nfrom sklearn.preprocessing import*\nfrom sklearn.ensemble import*\nfrom sklearn.neighbors import*\nfrom sklearn import svm\nfrom sklearn.naive_bayes import*\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n# %% [markdown] {\"id\":\"7_OBzOaoHGcA\"}\n# ---\n# <a name = Section4></a>\n# # **4. Data Acquisition & Description**\n# ---\n# [Go back to Index](#Section0)<br>   \n\n# %% [code] {\"id\":\"UxScFLFC1Eji\"}\ndata = pd.read_csv(filepath_or_buffer = 'https://raw.githubusercontent.com/insaid2018/Term-2/master/Projects/avocado.csv')\n\n# %% [code] {\"id\":\"kpOaMbg21Ejj\",\"outputId\":\"e73d7e0c-3673-49cc-d6ef-3b1cc7cf1255\"}\nprint('Data Shape:', data.shape)\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')\n\n# %% [code] {\"id\":\"38r4JO1b1Ejj\",\"outputId\":\"9934a9ed-0023-4441-c14d-4213f49d1ed3\"}\ndata.head()\n\n# %% [code] {\"id\":\"YC7k9k6e1Ejj\",\"outputId\":\"3c461e4d-52c7-465d-aba4-535f547c00f5\"}\ndata.describe()\n\n# %% [code] {\"id\":\"sC3aSmIL1Ejj\",\"outputId\":\"0d0bbea4-9b5c-4de5-cd0a-5312aa2e60d2\"}\ndata.dtypes\n\n# %% [markdown] {\"id\":\"PrGn04F31Ejg\"}\n# **To predict the future price of avocados depending on some variables / features we have such as**\n# \n# - Date - The date of the observation\n# - AveragePrice - the average price of a single avocado\n# - Total Volume - Total number of avocados sold (small Hass + Large Hass + XLarge Hass + Total Bags)\n# - 4046 - Total number of avocados with PLU 4046 sold\n# - 4225 - Total number of avocados with PLU 4225 sold\n# - 4770 - Total number of avocados with PLU 4770 sold\n# - Total Bags = Small Bags + Large Bags + XLarge Bags\n# - type - conventional or organic\n# - year - the year\n# - region - the city or region of the observation\n\n# %% [markdown] {\"id\":\"SNnIcfCxN2hP\"}\n# ---\n# <a name = Section5></a>\n# # **5. Data Pre-Processing**\n# ---\n# [Go back to Index](#Section0)<br>   \n# - This section is emphasised on performing data manipulation over unstructured data for further processing and analysis.\n# \n# - To modify unstructured data to strucuted data you need to verify and manipulate the integrity of the data by:\n#   - Handling missing data,\n# \n#   - Handling redundant data,\n# \n#   - Handling inconsistent data,\n# \n#   - Handling outliers,\n# \n#   - Handling typos\n\n# %% [markdown] {\"id\":\"FePapBKG1Ejj\"}\n# #### Null & Duplicate Entry check:\n\n# %% [code] {\"id\":\"veUKGxPp1Ejk\",\"outputId\":\"ee233593-485c-4713-8394-5c9879e015c5\"}\ndata.info()\n\n# %% [code] {\"id\":\"o2bRwUst1Ejk\",\"outputId\":\"a820c5eb-d9e2-4e06-91bb-589c778d4840\"}\ndata.isna().sum()\n\n# %% [code] {\"id\":\"R-tYsfiN1Ejk\",\"outputId\":\"42faf289-d0ec-4172-af1f-47d806134a2c\"}\nsns.heatmap(data.isnull());\n\n# %% [code] {\"id\":\"D26oOnr91Ejk\",\"outputId\":\"bc3d9206-7d36-47ab-a219-0365c550a66b\"}\nprint('total number of duplicate values : ',sum(data.duplicated()))\n\n# %% [markdown] {\"id\":\"H--_bVgN1Ejk\"}\n# **We don't have any null or duplicate value. Lets continue with the descriptive analysis and further. The first column gives reduntant index data, so lets drop it**\n\n# %% [code] {\"id\":\"vhR9ckXV1Ejl\"}\ndata=data.drop(['Unnamed: 0'], axis=1)\n\n# %% [code] {\"id\":\"bvwpVaZu1Ejl\",\"outputId\":\"bab9e99c-ac25-4690-de2d-74fd5f07a917\"}\ndata.head()\n\n# %% [code]\ndatam=pd.read_csv(filepath_or_buffer = 'https://raw.githubusercontent.com/insaid2018/Term-2/master/Projects/avocado.csv') # Archieving main dataset\n\n# %% [markdown] {\"id\":\"r3DC-G4y1Ejl\"}\n# #### String Check\n\n# %% [code] {\"id\":\"HNIuLHBV1Ejl\",\"outputId\":\"7c2f9ff7-d220-47db-86b0-a184b010b8b3\"}\ndata.select_dtypes('object').columns\n\n# %% [markdown] {\"id\":\"WWgxHcC01Ejm\"}\n# ## Outlier Detection\n\n# %% [markdown] {\"id\":\"ZYQdXyZ-1Ejm\"}\n# We can use **Standard Deviation** or **Quantile** to detect if any outliers\n\n# %% [code] {\"id\":\"V1NUFSyO1Ejm\",\"outputId\":\"de496218-915e-4b7a-c20e-5181f68ea517\"}\nsns.set_style(\"white\")\n\nplt.figure(figsize=(12,12))\nsns.distplot(data.AveragePrice)\nplt.title(\"Distribution of Average Price\",fontsize=12);\n\n# %% [markdown] {\"id\":\"hjk81H161Ejm\"}\n# #### <font color=\"blue\">Looks like positive skewed slightly\n\n# %% [code] {\"id\":\"KpBN7gkr1Ejm\",\"outputId\":\"f9f76e76-dbee-48f5-e729-b47171a92957\"}\nimport statsmodels.api as sm\nsm.qqplot(data.AveragePrice,line=\"s\")\n\n# %% [markdown] {\"id\":\"ZTKiTfEP1Ejm\"}\n# >- 1 std from mean : 68% of data included\n# - 2 std from mean : 95% of data included\n# - 3 std from mean : 99.7% of data included\n\n# %% [code] {\"id\":\"sinIk8Pw1Ejn\",\"outputId\":\"8134be0e-b030-4bde-fa82-1cdf0bd064b9\"}\nmean = data.AveragePrice.mean()\nstd = data.AveragePrice.std()\nlower, upper = mean-std*2,mean+std*2 # Use 2*std and it will exclude data that is not included in 95% of data\nprint(\"Lower Limit : {} Upper Limit : {}\".format(lower,upper))\n\n# %% [code] {\"id\":\"e5xIbpWo1Ejn\",\"outputId\":\"db37aa2a-0c3e-489a-dd0c-b9c6977f9dad\"}\noutliers = [x for x in data.AveragePrice if x < lower or x > upper]\nprint(\"Outlier values : {}\".format(outliers))\n\n# %% [markdown] {\"id\":\"5cm0pMmr1Ejn\"}\n# #### There is some data that is not included within 95% of data\n\n# %% [code] {\"id\":\"C4xOBWBP1Ejn\"}\ndf_exclude = data[(data.AveragePrice < upper) | (data.AveragePrice > lower)]\n\n# %% [code] {\"id\":\"6NCa5eqU1Ejn\",\"outputId\":\"25c8dd20-ca39-4840-8ea9-09cab7dc7a74\"}\ndf_exclude.head()\n\n# %% [code] {\"id\":\"kHb-XcZk1Ejn\",\"outputId\":\"006ce4c3-2d3f-49a9-d0dc-8f2a86e143d3\"}\ndf_exclude.shape\n\n# %% [code] {\"id\":\"SFm70-cY1Ejn\",\"outputId\":\"78b4fe8d-2b02-44ee-ce44-660732ec12ce\"}\ndata.shape\n\n# %% [markdown] {\"id\":\"c2Zxbocs1Ejo\"}\n# >- Q1 : Data that is located in 25% of total data\n# - Q2 : Median value of data\n# - Q3 : Data that is located in 75% of total data\n# \n# $IQR = Q3 - Q1$\n# \n# **Outlier:**\n# \n# $Upper bound : 1.5*IQR + Q3$\n# \n# $Lower bound : 1.5*IQR - Q1$\n\n# %% [code] {\"id\":\"wSSNUUL81Ejo\",\"outputId\":\"296866e5-c7e5-41b9-ab74-08b31eb276dc\"}\nquantile = np.quantile(data.AveragePrice,[0.25,0.5,0.75,1]) # Use numpy quantile\nIQR = quantile[2] - quantile[0] # Calculate IQR through third quantile - first quantile\nupper = 1.5*IQR + quantile[2]\nlower = quantile[0] - 1.5*IQR\n\nprint(\"Upper bound : {} Lower bound : {}\".format(upper,lower))\n\noutlier = [x for x in data.AveragePrice if x < lower or x>upper]\nprint(\"\\nOutlier values :\\n {}\".format(outliers))\n\n# %% [code] {\"id\":\"5c-vjmBH1Ejo\",\"outputId\":\"fb456869-c8ee-4639-9aa1-e13f42ba736e\"}\ndf_exclude2 = data[(data.AveragePrice > lower) | (data.AveragePrice < upper)]\ndf_exclude2\n\n# %% [markdown] {\"id\":\"BYskfFHg1Ejo\"}\n# ### Data Normalization\n# \n# It is important to check whether data follow normal distribution before we do modeling\n# There is one easy way to do normalization, use logarithmic scale\n\n# %% [code] {\"id\":\"lWFEKbRv1Ejo\",\"outputId\":\"9694a35b-ce8c-4977-c06e-17f908eeb3e9\"}\nlog_data = np.log(data.AveragePrice+1)\nsns.set_style(\"white\")\nplt.figure(figsize=(8,8))\nsns.distplot(log_data);\n\n# %% [code] {\"id\":\"EbrUzBya1Ejo\",\"outputId\":\"523df749-d32f-4da8-d576-ca9846b4205e\"}\nfig,ax = plt.subplots(1,2,figsize=(10,7))\nsm.qqplot(data.AveragePrice,line=\"s\",ax=ax[0])\nax[0].set_title(\"Before logarithmic\")\nsm.qqplot(log_data,line=\"s\",ax=ax[1])\nax[1].set_title(\"After logarithmic\");\n\n# %% [markdown] {\"id\":\"jXuKUX8K1Ejo\"}\n# #### It looks much closer to normal distribution after doing logarithmic\n\n# %% [markdown] {\"id\":\"S7omk2LK1Ejp\"}\n# ---\n# <a name = Section6></a>\n# # **6. EDA & Class Imbalance Check**                            \n# ---\n# [Go back to Index](#Section0)<br>\n\n# %% [markdown] {\"id\":\"a9HLJCe11Ejp\"}\n# ### Region\n\n# %% [code] {\"id\":\"uYDr1qPs1Ejp\",\"outputId\":\"8454289d-9402-4b10-fcd5-02a4a92f5257\"}\nlen(data.region.unique())\n\n# %% [code] {\"id\":\"KpzORxYR1Ejp\",\"outputId\":\"ae8b3305-a827-4c39-e3bf-7f6e21aa5f44\"}\ndata.groupby('region').size() \n\n# %% [markdown] {\"id\":\"1c1CNzVM1Ejp\"}\n# **There are ~338 observations from each region, dataset seems balanced, and there are 54 regions.**\n\n# %% [markdown] {\"id\":\"QxWV0XeB1Ejp\"}\n# ### The average prices by regions\n\n# %% [code] {\"id\":\"juGYjgJx1Ejp\",\"outputId\":\"d5a8885b-47bd-4e33-9746-2b5b663739b8\"}\nplt.figure(figsize=(12,5))\nplt.title(\"Distribution Price\")\nax = sns.distplot(data[\"AveragePrice\"], color = 'g')\n\n# %% [code] {\"id\":\"tuWlghCR1Ejp\",\"outputId\":\"c9a8ce54-dc57-44ab-894a-a1da558f8a36\"}\nsns.boxplot(y=\"type\", x=\"AveragePrice\", data=data, palette = 'pink');\n\n# %% [code] {\"id\":\"1eaifnao1Ejq\",\"outputId\":\"44df15b3-4e44-43e6-e3c6-f9d99baeb211\"}\n#Weight distribution of prices\nfig, ax = plt.subplots()\nfig.set_size_inches(10,5)\nsns.violinplot(data.dropna(subset = ['AveragePrice']).AveragePrice);\n\n# %% [markdown] {\"id\":\"jt_wjfEZ1Ejq\"}\n# Organic avocados are more expensive. This is obvious, because their cultivation is more expensive and we all love natural products and are willing to pay a higher price for them. But it is likely that the price of avocado depends not only on the type. Let's look at the price of avocado from different regions in different years. Let's start with organic avocados.\n\n# %% [code] {\"id\":\"SPzc75LW1Ejq\",\"outputId\":\"bb57daa6-9e7e-4714-c5c8-649b8396d042\"}\nplt.figure(figsize=(15,15))\n\nplt.title(\"Avgerage Price of Avocado by Region\")\n\nsns.barplot(x=\"AveragePrice\",y=\"region\",data=data)\n\nplt.show()\n\n# %% [markdown] {\"id\":\"WJa3Roye1Ejq\"}\n# Seems there are some regions which are US States (say California) and US Cities (say San Francisco) of that State or just Cities. Also there is a region as \"TotalUS\"; \"West\".\n\n# %% [code] {\"id\":\"fyexiquD1Ejq\",\"outputId\":\"428e88d6-d2fb-4458-a990-6b25d68f6b6e\",\"scrolled\":true}\nfig, ax = plt.subplots(1, 2, figsize=(18,5))\n\nregionP = data.groupby('region')['AveragePrice'].mean()\n\nexpensive = regionP.sort_values(ascending = False).iloc[:10]\ncheap = regionP.sort_values().iloc[:10]\n\nsns.barplot(x='AveragePrice', y='region', data = data, order=expensive.index, ci=None, palette='Greens_r', ax=ax[0])\nsns.barplot(x='AveragePrice', y='region', data = data, order=cheap.index, ci=None, palette='Greens_r', ax=ax[1])\n\nplt.show()\n\ncheap = pd.DataFrame(cheap).reset_index()\nexpensive = pd.DataFrame(expensive).reset_index()\n\nprint('the most expensive datacados can be found in {} '.format(list(expensive.iloc[:5,0])))\nprint('the cheapest datacados can be found in {} '.format(list(cheap.iloc[:5,0])))\n\n# %% [markdown]\n# **The most expensive avocados can be found in ['HartfordSpringfield', 'SanFrancisco', 'NewYork', 'Philadelphia', 'Sacramento']** \n# \n# **The cheapest avocados can be found in ['Houston', 'DallasFtWorth', 'SouthCentral', 'CincinnatiDayton', 'Nashville']**\n\n# %% [code] {\"id\":\"wN0xYokU1Ejq\",\"outputId\":\"dde65414-f795-4468-947b-9aeea156f72a\"}\nfig, ax = plt.subplots(1, 2, figsize=(18,5))\n\ndataStates = data[data['region'] !='TotalUS']\n\nregionV = dataStates.groupby('region')['Total Volume'].sum()\n\nmost = regionV.sort_values(ascending = False).iloc[:10]\nleast = regionV.sort_values().iloc[:10]\n\nsns.barplot(x='Total Volume', y='region', data = dataStates, order=most.index, ci=None, palette='Greens_r', ax=ax[0])\nsns.barplot(x='Total Volume', y='region', data = dataStates, order=least.index, ci=None, palette='Greens_r', ax=ax[1])\n\nplt.show()\n\nmost = pd.DataFrame(most).reset_index()\nleast = pd.DataFrame(least).reset_index()\n\nprint('States with the the biggest demand are {} '.format(list(most.iloc[:5,0])))\nprint('States with the least demand are {} '.format(list(least.iloc[:5,0])))\n\n# %% [markdown] {\"id\":\"Pql1fmC41Ejr\"}\n# ### Dates & Seasonality check\n\n# %% [markdown] {\"id\":\"gwhMcHLE1Ejr\"}\n# We have two columns which are 'Date' and 'year', being year the extracted year of date. To make the analysis easier, let's extract day and month out of 'Date' and see each value separately. That way, we are also going to have two more potentially usefull columns: day and month\n\n# %% [code] {\"id\":\"1j6P1UDC1Ejr\",\"outputId\":\"20b7f9f1-21cd-4712-ad52-24661ae09f69\"}\nfrom datetime import datetime\ndata['Date'] = data['Date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\ndata['month'] = data['Date'].dt.month\ndata['day'] = data['Date'].dt.day\n# monday = 0\ndata['day of week'] = data['Date'].dt.dayofweek\ndates = ['year', 'month', 'day', 'day of week']\ndata[dates]\n\n# %% [code] {\"id\":\"2hgRUiue1Ejr\",\"outputId\":\"1b1977f7-acac-44c5-a6e4-2aa915e275d9\"}\nfig, ax = plt.subplots(2,2, figsize=(20,10))\n\nsns.countplot('year', data=data, ax=ax[0,0], palette='BuGn_r')\nsns.countplot('month', data=data, ax=ax[0,1], palette='BuGn_r')\nsns.countplot('day', data=data, ax=ax[1,0], palette='BuGn_r')\nsns.countplot('day of week', data=data, ax=ax[1,1], palette='BuGn')\n\nplt.show()\n\n# %% [markdown] {\"id\":\"PSpCV6cx1Ejr\"}\n# **Year**\n# \n# - 2015, 2016, 2017 have almost the same values\n# - 2018 is the lowest, looks like the avocados should have ended in the begining of 2018\n# \n# **Month**\n# \n# - Shows a descending pattern, This could be because of the same reason as year: 2018 ended in the begging of the year and, therefore, the first months have more entries\n# \n# **Day & day of week**\n# \n# - We can see that the day chart has a repeating trend, and this is because of the day that the data was always recorded: day 6 (Sunday).\n# - The data was, therefore, recorded weekly, 'day of week' becomes redundant and we can eliminate it.\n\n# %% [code] {\"id\":\"jD7Zd5Mm1Ejr\"}\ndata.drop('day of week', axis=1, inplace=True)\n\n# %% [markdown] {\"id\":\"VcqoGhop1Ejs\"}\n# <font color ='blue'>\n# \n# - type' has to categories and is balanced, could be used as a classifier in model building\n# - 'region' has 54 unique values and is perfectly balanced, could be hot encoded for model building\n# - 'avg' price shows and pretty normal distribution and looks tentative for target variable for regression model\n# - units sold columns show similar data which is similarly distributed, log formulas could be used to increase model performance\n# - 'dates' is evenly distributed till 2018 and shows that the data was recorded on a weekly basis every Sunday\n\n# %% [code] {\"id\":\"r1JaaP-x1Ejs\",\"outputId\":\"66635663-518f-4f3f-eb61-f691c0c69ce5\"}\nfig, ax = plt.subplots(2, 1, figsize=(23,10))\n\ndata['year_month'] = data['Date'].dt.to_period('M')\ngrouped = data.groupby('year_month')[['AveragePrice', 'Total Volume']].mean()\n\nax[0].plot(grouped.index.astype(str), grouped['AveragePrice'])\nax[0].tick_params(labelrotation=90)\nax[0].set_ylabel('AveragePrice')\n\n\nax[1].plot(grouped.index.astype(str), grouped['Total Volume'])\nax[1].tick_params(labelrotation=90)\nax[1].set_ylabel('Total Volume')\n\nplt.show()\n\n# %% [markdown] {\"id\":\"DfWEKW5A1Ejs\"}\n# <font color ='blue'>\n# \n# - From the graphic we can tell that the, first of all, average price and total volume move in different direction\n# - Total volume has a spike at the beginning of the year. On the other hand, average price drops at the beginning of the year\n# - These drops and spikes are a sign of seasonality and that could help in forecasting\n\n# %% [code] {\"id\":\"dcJcrfVn1Ejs\",\"outputId\":\"8f77af6d-4e6b-493f-ea95-f6cd897a70b0\"}\nfig, ax = plt.subplots(1, 2, figsize=(12,5))\n\ndata['quarter'] = data['Date'].dt.quarter\n\n\nsns.barplot(x='quarter', y='Total Volume', data=data, palette='Greens_r', ci=None, ax=ax[0])\nsns.barplot(x='quarter', y='AveragePrice', data=data, palette='Greens_r', ci=None, ax=ax[1])\n\n\nplt.show()\n\nquarter = data.groupby('quarter')[['Total Volume', 'AveragePrice']].mean()\ndisplay(quarter)\n\n# %% [markdown] {\"id\":\"0BtNIQQI1Ejs\"}\n# - So we see that in the first quarter of the year sales are better than in other quarters and prices are the lowest.\n# - After the first quarter, sales decrease and prices grow. Given the popularity of avos, businesses should be considering importing more avos when they are not produced in the country, a big oportunity for business-men from both countries.\n\n# %% [code] {\"scrolled\":true}\ndata.head()\n\n# %% [markdown] {\"id\":\"1WFXIJ8y1Ejs\"}\n# ### Type\n\n# %% [code] {\"id\":\"IAXdclzm1Ejs\",\"outputId\":\"8076b197-1c4b-4a06-ca1b-3c122b3af272\"}\nprint(len(data.type.unique()))\n\ndata.groupby('type').size()\n\n# %% [markdown] {\"id\":\"xVCEMx2v1Ejt\"}\n# Types of avocados are also balanced since the ratio is nearly 0.5 each.\n\n# %% [markdown] {\"id\":\"Rmurq3nB1Ejt\"}\n# ### The average prices of avocados by types\n\n# %% [code] {\"id\":\"KaLStskd1Ejt\",\"outputId\":\"4ed9f154-1b0c-4366-b3b4-28698860fde6\"}\nplt.figure(figsize=(5,7))\n\nplt.title(\"Avg.Price of Avocados by Type\")\n\nsns.barplot(x=\"type\",y=\"AveragePrice\",data= data)\n\nplt.show()\n\n# %% [code] {\"id\":\"vAaWos4i1Ejt\",\"outputId\":\"d9de83d0-2c50-44f4-ee38-c29f94f0b3c1\"}\nplt.figure(figsize=(18,10))\nsns.lineplot(x=\"month\", y=\"AveragePrice\", hue='type', data=data);\n\n\n# %% [code] {\"id\":\"58HbYngj1Ejt\",\"outputId\":\"d6f9b856-4493-4dba-cedd-1941a8560139\"}\ndata['month'].head()\n\n# %% [code] {\"id\":\"vi36M7Ya1Ejt\",\"outputId\":\"7af47954-6c20-4dca-fb7d-d9e8a228fe63\"}\ndata['month'] = data['month'].replace({1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', \n                                   6: 'June', 7: 'July', 8: 'August', 9: 'September', 10: 'October', \n                                   11: 'November', 12: 'December'})\nax = sns.catplot(x=\"month\", y=\"AveragePrice\", hue=\"type\", \n            kind=\"box\", data=data, height=8.5, linewidth=2.5, aspect=2.8,palette=\"Set2\");\n\n# %% [code] {\"id\":\"FFxgUL5I1Ejt\",\"outputId\":\"f4b97311-7a06-45d1-9344-b3adffde77cb\"}\nplt.figure(figsize=(18,10))\nsns.lineplot(x=\"month\", y=\"AveragePrice\", hue='year',  data=data)\nplt.show()\n\n# %% [code] {\"id\":\"L4LAMJ_p1Ejt\",\"outputId\":\"7f4980e7-fb03-424d-8453-7aebfdb82bc6\"}\nfig, ax = plt.subplots(1, 2, figsize=(14,5))\n\nsns.barplot(x='type', y='AveragePrice', data=data, palette='Set3', ax=ax[0])\nsns.barplot(x='type', y='Total Volume', data=data, palette='Set3', ax=ax[1], estimator=sum, ci=None)\nplt.show()\n\ndisplay(data.groupby('type')['AveragePrice'].mean())\ndisplay(data.groupby('type')['Total Volume'].sum())\n\n# %% [markdown] {\"id\":\"Q1_8pzdu1Eju\"}\n# **Convential is cheaper than organic, but surprisingly, conventional destroyed organic sells. So conventional avos are performing quite well and organic are being left behind, but is organic at least geaining popularity?**\n\n# %% [code] {\"id\":\"B9-15Ene1Eju\",\"outputId\":\"915fa84a-bc63-431a-e48b-ccfe7aa35adb\"}\nfrom matplotlib.colors import DivergingNorm\nfig, ax = plt.subplots(2, 1, figsize=(23,12))\nfig.tight_layout(pad=8)\n\n\ngroup = data.groupby(['type', 'year_month'])['Total Volume'].sum()\n\norganic = group['organic']\norganic = pd.DataFrame(organic)\norganic['Total Volume % change'] = np.round(organic['Total Volume'].pct_change() * 100, 2)\n\nconventional = group['conventional']\nconventional = pd.DataFrame(conventional)\nconventional['Total Volume % change'] = np.round(conventional['Total Volume'].pct_change() * 100, 2)\n\nnorm = DivergingNorm(vmin=organic['Total Volume % change'].min(), vcenter=0, vmax=organic['Total Volume % change'].max())\ncolors = [plt.cm.RdYlGn(norm(c)) for c in organic['Total Volume % change']]\nsns.barplot(x=organic.index, y=organic['Total Volume % change'], data=organic, ax=ax[0], palette=colors)\n\nnorm = DivergingNorm(vmin=conventional['Total Volume % change'].min(), vcenter=0, vmax=conventional['Total Volume % change'].max())\ncolors = [plt.cm.RdYlGn(norm(c)) for c in conventional['Total Volume % change']]\nsns.barplot(x=conventional.index, y=conventional['Total Volume % change'], data=conventional, ax=ax[1], palette=colors)\n\n\nax[0].tick_params(labelrotation=90)\nax[0].set_title('Organic Percentage Change in Sales', fontsize=15)\n\nax[1].tick_params(labelrotation=90)\nax[1].set_title('Conventional Percentage Change in Sales', fontsize=15)\n\nplt.show()\n\nconventional['Total Volume % change'].mean()\nprint(\"The sum of percentage change of Organic is: {}\".format(np.around(organic['Total Volume % change'].sum(), 2)))\nprint(\"The sum of percentage change of Conventional is: {}\".format(np.around(conventional['Total Volume % change'].sum(), 2)))\n\n# %% [markdown] {\"id\":\"gbmUauod1Eju\"}\n# Is hard to tell from the graphic alone but if we sum every percentage change we find that organic has a bigger growth overall with 200.48 against 137.02 of conventional.\n# \n# Let's add some business strategy concepts to refine strategy and conclusions here.\n# \n# The BCG matix is a model that evaluates how a business is performing according its growth and market share. It has for dimensions:\n# \n# Dogs: These are products with low growth or market share.\n# Question marks or Problem Child: Products in high growth markets with low market share.\n# Stars: Products in high growth markets with high market share.\n# Cash cows: Products in low growth markets with high market share.\n# Organic might be having way smaller sales than conventional, but its growing rate (higher than conventional) is a good sign to keep producing the organic avos and it already has a market. This is a healthy indicator for businesses.Then, organic is a Star in the BCG matrix. A suggestion would then be to have a business growth strategy with them: technologies and methods that produce more and cheaper, promotion and importations.\n# \n# Conventional avos are too succesfull and have an already stablished business infrastructure. Therefore, conventional are Cash cows in the BCG matrix, and businesses should keep producing them at the same or higher rate.\n\n# %% [code] {\"id\":\"DGkRftDn1Eju\",\"outputId\":\"dd1b374a-0086-45f2-ae81-ac3ec20c0f63\"}\nregion_list=list(data.region.unique())\naverage_price=[]\n\nfor i in region_list:\n    x=data[data.region==i]\n    region_average=sum(x.AveragePrice)/len(x)\n    average_price.append(region_average)\n\ndata1=pd.DataFrame({'region_list':region_list,'average_price':average_price})\nnew_index=data1.average_price.sort_values(ascending=False).index.values\nsorted_data=data1.reindex(new_index)\n\nplt.figure(figsize=(24,10))\nax=sns.barplot(x=sorted_data.region_list,y=sorted_data.average_price)\n\nplt.xticks(rotation=90)\nplt.xlabel('Region')\nplt.ylabel('Average Price')\nplt.title('Average Price of Avocado According to Region',fontsize=18);\n\n# %% [code] {\"id\":\"h58_BWwM1Eju\",\"outputId\":\"e2c72ff7-8371-43f2-cbb6-cb4f560cac0e\"}\nfilter1=data.region!='TotalUS'\ndata1=data[filter1]\n\nregion_list=list(data1.region.unique())\naverage_total_volume=[]\n\nfor i in region_list:\n    x=data1[data1.region==i]\n    average_total_volume.append(sum(x['Total Volume'])/len(x))\ndata3=pd.DataFrame({'region_list':region_list,'average_total_volume':average_total_volume})\n\nnew_index=data3.average_total_volume.sort_values(ascending=False).index.values\nsorted_data1=data3.reindex(new_index)\n\nplt.figure(figsize=(22,10))\nax=sns.barplot(x=sorted_data1.region_list,y=sorted_data1.average_total_volume)\n\nplt.xticks(rotation=90)\nplt.xlabel('Region')\nplt.ylabel('Average of Total Volume')\nplt.title('Average of Total Volume According to Region',fontsize=18);\n\n# %% [code] {\"id\":\"90E9o-yG1Eju\",\"outputId\":\"aa94f678-1a17-4b5f-84a9-0f7b1e92e779\"}\ng = sns.factorplot('AveragePrice','region',data=data,\n                   hue='year',\n                   size=18,\n                   aspect=0.7,\n                   palette='magma',\n                   join=False,\n              )\n\n# %% [markdown] {\"id\":\"QmnpSxzh1Eju\"}\n# Oh San Francisco, 2017..... In 2017, organic avocados were very expensive :( Search in Google gave result on this question. In 2017, there was a shortage of avocados. That explains the price increase!\n\n# %% [code] {\"id\":\"C5ho1VNV1Ejv\",\"outputId\":\"f9d2650f-e13e-4283-9de5-d07c0ba1a361\"}\nmask = data['type']=='conventional'\ng = sns.factorplot('AveragePrice','region',data=data[mask],\n                   hue='year',\n                   size=13,\n                   aspect=0.8,\n                   palette='magma',\n                   join=False,\n              )\n\n# %% [markdown] {\"id\":\"ZyJyojfj1Ejv\"}\n# For obvious reasons, prices are lower. The situation with the price increase in 2017 also affected this type of avocado.\n# \n# Organic avocado type is more expensive. And avocado is generally more expensive with each passing year\n\n# %% [code] {\"id\":\"ddN02GTq1Ejv\",\"outputId\":\"cc5aaef3-c9f7-44f0-c9c4-a48c2d5e7d21\"}\nnumbers = list(data.select_dtypes(['float64', 'int64']).keys())\n\n# removing years\nnumbers.remove('year')\n\ndata[numbers].hist(figsize=(20,10), color='green', edgecolor='white')\n\nplt.show()\n\ndisplay(data[numbers].describe())\n\n# %% [markdown] {\"id\":\"SS8NBnDI1Ejv\"}\n# <font color='blue'> **Average Price**\n# \n# - Is the most normal distribution. Mean and median are really closed, which means the distribution is not severly influenced by outliers. Still, it is a bit skewed to the right, the mean being bigger than the median reflects that. Remaining features\n# - The remaining features are severely influenced by outliers, most of the values are located in the first bin of the histograms and the meean is way bigger than the median.\n# - These features seem to follow the same distribution, which makes sense since the information (quantity sold) is similar\n# \n# Lets take the outliers out of the quantities to see if we can find a more normal distribution <font> \n\n# %% [code] {\"id\":\"vIPqO9C31Ejv\",\"outputId\":\"2579c669-5e9d-4a6c-cdaa-438607f5c00e\"}\ndata_o = data[data['Total Volume']<50000]\ndata_o[numbers].hist(figsize=(20,10), color='green', edgecolor='white')\n\nplt.show()\n\n# %% [markdown] {\"id\":\"Se0FVXld1Ejv\"}\n# These kind of distributions, where most of the values are located in lower values and then descends, is really common and could be represented in a different way through log formulas to make it more 'normal' and useful for a model, like regression models, without getting rid of outliers.\n# \n# A example below with Total Volume.\n# \n# Refer **Outlier Detection sction** for details\n\n# %% [code] {\"id\":\"8BiihFoA1Ejv\",\"outputId\":\"cea6aaed-e3a7-4b06-c4ad-78e62d941764\"}\nTotalLog = np.log(data['Total Volume'] + 1)\nTotalLog.hist(color='green', edgecolor='white');\n\n# %% [markdown] {\"id\":\"QP_Azn8C1Ejw\"}\n# ### Correlation\n\n# %% [code] {\"id\":\"hEXzdYIx1Ejw\",\"outputId\":\"4531d3eb-dd47-4e6b-d3bc-504084019b26\"}\nplt.figure(figsize=(12,6))\nsns.heatmap(data.corr(),cmap='coolwarm',annot=True);\n\n# %% [markdown] {\"id\":\"f-6iBI-Q1Ejw\"}\n# **Observation :**\n# \n# There is a high correlation between pairs:\n# \n# 4046 & total volume (0.98)\n# 4225 & total volume (0.97)\n# 4770 & total volume (0.87)\n# total bags & total volume (0.96)\n# small bags & total bags (0.99)\n# etc\n# \n# 4046 avocados are the most preferred/sold type in the US and customers tend to buy those avocados as bulk, not bag.\n# \n# Retailers want to increase the sales of bagged avocados instead of bulks. They think this is more advantageous for them.\n# Total Bags variable has a very high correlation with Total Volume (Total Sales) and Small Bags, so we can say that most of the bagged sales comes from the small bags.\n\n# %% [code] {\"id\":\"VUBdtcX-1Ejw\"}\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ndicts = {}\n\nlabel.fit(data.type.drop_duplicates()) \ndicts['type'] = list(label.classes_)\ndata.type = label.transform(data.type) \n\n# %% [code] {\"id\":\"vjYj-I7u1Ejw\",\"outputId\":\"caa28a52-13c6-4eee-a87a-fa0194c0668f\"}\ncols = ['AveragePrice','type','year','Total Volume','Total Bags']\ncm = np.corrcoef(data[cols].values.T)\nsns.set(font_scale = 1.7)\nhm = sns.heatmap(cm,cbar = True, annot = True,square = True, fmt = '.2f', annot_kws = {'size':15}, yticklabels = cols, xticklabels = cols)\n\n# %% [markdown] {\"id\":\"77OWcT6g1Ejw\"}\n# The price of avocado is influenced by the type. Logically. We also see a strong correlation between the features: \"Total Bags\" and \"Total Volume\". Also, if you look at the correlation of all the features, you will notice that strongly correlated Small Bags,Large Bag. It is logical but can create problems if we go to predict the price of avocado. \n\n# %% [code] {\"id\":\"_krLcWOe1Ejw\",\"outputId\":\"2673d2bf-9b63-4ea5-9978-1bd49a92db63\"}\nfig, ax = plt.subplots(1, 2,figsize=(40,15))\n\ndata_o = data[data['Total Volume']<50000]\n\nsns.heatmap(data.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True, ax=ax[0])\nax[0].set_title('With outliers', fontsize=25)\n\nsns.heatmap(data_o.corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True, ax=ax[1])\nax[1].set_title('Without outliers', fontsize=25)\n\nplt.show()\n\n# %% [markdown] {\"id\":\"gy6keRQD1Ejx\"}\n# - We are going to take the strongest relationship out of the volume variable and the strongest out of a date variable\n# - We are going to take the relationships with AveragePrice, out of both heatmaps, since is our target variable for the regression model\n\n# %% [code] {\"id\":\"99QqPhFP1Ejx\",\"outputId\":\"c02cccb7-50f7-40c2-dafc-5c25a81da95b\"}\nfig, ax = plt.subplots(2, 2, figsize=(20,10))\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.1})\nsns.set_context(\"paper\", font_scale=0.9)    \nsns.scatterplot(x='4046', y='AveragePrice', data=data, hue='type', ax=ax[0,0])\nsns.scatterplot(x='Large Bags', y='AveragePrice', data=data_o, hue='type', ax=ax[0,1])\nsns.scatterplot(x='month', y='AveragePrice', data=data, hue='type', ax=ax[1,0])\nsns.scatterplot(x='month', y='AveragePrice', data=data_o, hue='type', ax=ax[1,1])\n# Labels and clean up on the plot                                                                                                                                                                                                                                                                                              \nplt.xticks(rotation=90)                                                               \nplt.tight_layout()\n#plt.savefig('test.pdf', bbox_inches='tight') ;\n\n# %% [markdown] {\"id\":\"-ZGOYVN71Ejx\"}\n# <font color='blue'>\n#     \n# - An important insight here is that we can't take the outliers out since all of them correspond to the conventional type, which means that conventional avocados sell way more than organic avocados\n#     \n# - There doesn't seem to be a relationship between month and AveragePrice, what we can see in this graph is that the average price of conventional avocados is way smaller that the organic. We are going to take a closer look at this in the further sections\n#     \n# - There is an expected decreasing trend for both types: the more units were sold, the less the average price is, we are going to take a closer look at this later as well.\n#     \n# - Perhaps a better way of representing the data is not by taking out the outliers but by normilizing the data, let's try that now with AveragePrice and 4046\n\n# %% [code] {\"id\":\"VxMRdbpH1Ejx\",\"outputId\":\"c838fc2f-b1f7-4b9a-edd6-fdd8cf39e7f1\"}\nscaler = Normalizer()\nscaler.fit(data[['4046', 'AveragePrice']].values)\ndata['4046_scaled'] = scaler.transform(data[['4046', 'AveragePrice']].values)[:,0]\ndata['AveragePrice_scaled'] = scaler.transform(data[['4046', 'AveragePrice']].values)[:,1]\n\nsns.regplot(x='4046_scaled', y='AveragePrice_scaled', data=data, color='g')\nplt.show()\n\n# %% [markdown] {\"id\":\"26YCSulR1Ejx\"}\n# **We now know that both the regression and classification is possible since there is a clear tendency**\n\n# %% [markdown] {\"id\":\"_hbvkfM51Ejx\"}\n# <font color='blue'> *As we already see the field descriptions, so for our training we are interested only in fields as below*\n\n# %% [code] {\"id\":\"Z310bKCf1Ejx\"}\n# Specifying dependent and independent variables\n\nX = data[['4046', '4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region']]\nY = data['AveragePrice']\ny=np.log1p(Y)\n\n# %% [code] {\"id\":\"FyaD0MAG1Ejx\",\"outputId\":\"d144a8fd-fe9a-4e4b-dbcf-20fe083be3c8\"}\nX.head()\n\n# %% [code] {\"id\":\"PUNY7TDP1Ejy\",\"outputId\":\"ca2db81c-a6af-4b8e-f907-07e46f39879f\"}\nY.head()\n\n# %% [markdown] {\"id\":\"BjKuNEe31Ejy\"}\n# ### Labeling the categorical variables\n\n# %% [code] {\"id\":\"NVlk5fdx1Ejy\",\"outputId\":\"f7886bef-5f4f-4949-e1f1-e0e0fa5182aa\"}\n# X_labelled = pd.get_dummies(X[[\"type\",\"region\"]], drop_first = True)\n# X_labelled.head()\n\nX = pd.get_dummies(X, prefix=[\"type\",\"region\"], columns=[\"type\",\"region\"], drop_first = True)\nX.head()\n\n# %% [code] {\"id\":\"I9ytEXEa1Ejy\",\"outputId\":\"d2213821-54be-4745-c80b-37a43bd16c8c\"}\nprint(X.columns)\n\n# %% [markdown] {\"id\":\"H_Oryljd1Ejy\"}\n# ## Split into Train and Valid set\n\n# %% [code] {\"id\":\"aSXcfc151Ejy\"}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\n# %% [code] {\"id\":\"OHPhyDT31Ejy\"}\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.3, random_state = 99)\n\n# %% [code] {\"id\":\"aL1jnfP61Ejz\",\"outputId\":\"144736dc-37f5-49b8-a140-8a9afd055e51\"}\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n\n# %% [markdown] {\"id\":\"PkYBZg5t1Ejz\"}\n# ## Training the Model\n\n# %% [markdown] {\"id\":\"SDI9F6IV1Ejz\"}\n# ### Multiple Linear Regression\n\n# %% [code] {\"id\":\"ulvGLWNc1Ejz\",\"outputId\":\"8fd71631-1b9e-4737-ca28-08712b282609\"}\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\nprint(\"R2 of Linear Regresson:\", lr.score(X_train,y_train) )\nprint(\"----- Prediction Accuracy-----\")\nprint('MAE: ',metrics.mean_absolute_error(y_valid, lr.predict(X_valid)))\nprint('MSE: ',metrics.mean_squared_error(y_valid, lr.predict(X_valid)))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_valid, lr.predict(X_valid))))\n\n# %% [markdown]\n# **R2 of Linear Regresson: 0.56176162598828**\n# \n# **----- Prediction Accuracy-----**\n# \n# **MAE:  0.20301652029791611**\n# \n# **MSE:  0.07278245038216843**\n# \n# **RMSE: 0.2697822276988765**\n\n# %% [code] {\"id\":\"meOU2RmJ1Ejz\",\"outputId\":\"edfa736f-d72b-49d7-c22a-0622aaff7e57\"}\n# Creating a Histogram of Residuals\nplt.figure(figsize=(6,4))\nsns.distplot(y_valid - lr.predict(X_valid))\nplt.title('Distribution of residuals')\nplt.show()\n\n# %% [code] {\"id\":\"_l0Z6dte1Ejz\",\"outputId\":\"1d5c5ea2-8502-451d-efea-d02a29d3b3aa\"}\nplt.scatter(y_valid,lr.predict(X_valid));\n\n# %% [markdown] {\"id\":\"UOLEYfmc1Ejz\"}\n# ## Support Vector Regression\n\n# %% [code] {\"id\":\"fyGZP9Ze1Ejz\"}\nfrom sklearn.svm import SVR\n\n# %% [markdown] {\"id\":\"4gtTitfG1Ej0\"}\n# #### *let's first choose the best kernel for our data out of provided kernels.*\n\n# %% [code] {\"id\":\"PPz1xOws1Ej0\"}\n#clf = svm.SVR(kernel = 'linear')\n#clf.fit(X_train, y_train)\n#confidence = clf.score(X_train, y_train)\n#print(k,confidence)\n\n# %% [code] {\"id\":\"XvhgbF5c1Ej0\"}\n#for k in ['linear','poly','rbf','sigmoid']:\n#    print(\"Running for k as \", k)\n#    clf = svm.SVR(kernel=k)\n#    clf.fit(X_train, y_train)\n#    confidence = clf.score(X_train, y_train)\n#    print(k,confidence)\n\n# %% [markdown] {\"id\":\"n8riKyyk1Ej0\"}\n# ### Parameter Tuning or Hyperparameter\n\n# %% [markdown] {\"id\":\"SaVRQvSn1Ej0\"}\n# Intuitively, the *gamma* defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’.\n# \n# The *C* parameter trades off correct classification of training examples against maximization of the decision function’s margin.\n# \n# For larger values of *C*, a smaller margin will be accepted if the decision function is better at classifying all training points correctly.\n# \n# A lower *C* will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy.\n# \n# In other words *C* behaves as a **regularization parameter** in the SVM.\n\n# %% [code] {\"id\":\"MpvdIXBX1Ej0\",\"outputId\":\"b7b80135-9b16-4b4a-f2a4-8c8c3587b197\"}\nsvr = SVR(kernel='rbf', C=1, gamma= 0.5)   # Parameter Tuning to get the best accuracy\n\nsvr.fit(X_train,y_train)\nprint(svr.score(X_train,y_train))\n\n# %% [code] {\"id\":\"CxwNjybH1Ej1\"}\nfrom math import sqrt \n\n# %% [code] {\"id\":\"9lzJ_glQ1Ej1\",\"outputId\":\"a78f0f03-921f-4865-f747-0cbde79c3644\"}\n# calculate RMSE\nerror = sqrt(metrics.mean_squared_error(y_valid,svr.predict(X_valid))) \nprint('RMSE value of the SVR Model is:', error)\n\n# %% [code] {\"id\":\"-Nh-j9Dd1Ej1\",\"outputId\":\"4547a5ca-87cd-43b2-a50b-1aa7c4c12721\"}\n# Creating a Histogram of Residuals\nplt.figure(figsize=(6,4))\nsns.distplot(y_valid - svr.predict(X_valid))\nplt.title('Distribution of residuals')\nplt.show()\n\n# %% [code] {\"id\":\"F16nyUaY1Ej1\",\"outputId\":\"1a7eb878-1ee2-4434-b280-64216ba40cc7\"}\nplt.scatter(y_valid,svr.predict(X_valid));\n\n# %% [code] {\"id\":\"NY29_2qs1Ej1\",\"outputId\":\"6dc45e03-8f30-454d-e77b-3d0e1be57b2f\"}\n# Linear Regression RMSE : \nprint('RMSE value of the Linear Regr : ',round(np.sqrt(metrics.mean_squared_error(y_valid, lr.predict(X_valid))),4))\n\n# SVR RMSE               : \nprint('RMSE value of the SVR Model   : ',round(np.sqrt(metrics.mean_squared_error(y_valid, svr.predict(X_valid))),4))\n\n# %% [code]\ndata=data.drop(['Date'], axis=1)\n\n# %% [code]\ndata_dt=data # for decision tree alogorithm\n\n# %% [code] {\"id\":\"CyOOXR-u1Ej1\"}\ndata=data.drop(['year_month'], axis=1)\n\n# %% [markdown] {\"id\":\"ZxGIE9k_1Ej2\"}\n# ## Train & Validation\n\n# %% [markdown] {\"id\":\"TkhVyWdP1Ej2\"}\n# As we are predicting the price of the avocados we are going to put the prices column in the Y and rest of the data in X\n\n# %% [code] {\"id\":\"XSWKdjdX1Ej2\"}\nX=datam.drop('AveragePrice',1)\ny=datam['AveragePrice']\n\n# %% [code] {\"id\":\"TkXZwwkG1Ej2\",\"outputId\":\"aa8173f0-7512-407e-8882-5d8db0a7d51e\"}\nprint('shape of X and y respectively :',X.shape,y.shape)\n\n# %% [code]\nX.head()\n\n# %% [markdown] {\"id\":\"R2-fURz21Ej2\"}\n# #### *performing a 80-20 train test split over the dataset.*\n\n# %% [code] {\"id\":\"3LOrMRo61Ej2\"}\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n\n# %% [code] {\"id\":\"VJv0cK3b1Ej2\",\"outputId\":\"5fbb7289-8704-4108-ad73-2c565578d176\"}\nprint('shape of X and y respectively(train) :',X_train.shape,y_train.shape)\nprint('shape of X and y respectively(test) :',X_test.shape,y_test.shape)\n\n# %% [code]\ncols=X_train.columns\n\n# %% [markdown] {\"id\":\"Yw5KWcJJ1Ej3\"}\n# ### Preprocessing\n\n# %% [markdown] {\"id\":\"wf2zbjw41Ej3\"}\n# ### <font color=\"green\">Encoding\n\n# %% [markdown] {\"id\":\"qfTwZDeu1Ej3\"}\n# #### *Encoding all the categorical columns to dig deep into the data.*\n\n# %% [code] {\"id\":\"WWzvrPPr1Ej3\"}\nscaler=LabelEncoder()\n\n# %% [code] {\"id\":\"hpeVJ-qc1Ej3\",\"outputId\":\"39303745-a486-4610-a24e-70bc373d49af\"}\nfor col in X_train.columns:\n    if datam[col].dtype=='object':\n        X_train[col]=scaler.fit_transform(X_train[col])\n        X_test[col]=scaler.transform(X_test[col])\n\n# %% [code] {\"id\":\"t4KKpYX11Ej3\",\"outputId\":\"a100288f-29f8-483e-8c20-99f5d15b489a\"}\nX_train.head()\n\n\n# %% [code]\nX_train.shape\n\n# %% [markdown] {\"id\":\"Mz3-mHdS1Ej4\"}\n# ### <font color=\"green\">Variance thresholding\n\n# %% [markdown] {\"id\":\"mT_SoY-x1Ej4\"}\n# #### *Now after encoding the dataframe we have to omit the columns which are not contributing any pattern or key for finding good accuracy. That means we are going to drop the columns which have less variance than 0.1*\n\n# %% [code] {\"id\":\"dXzttupa1Ej4\"}\nscaler=VarianceThreshold(0.1)\n\n# %% [code] {\"id\":\"fF0S6Mjv1Ej4\",\"outputId\":\"a60fc8b9-f200-43b0-8f48-0d8434133990\",\"scrolled\":true}\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n\n# %% [code] {\"id\":\"GLRdWNk41Ej4\"}\nprint('shape of X (train) :',X_train.shape)\nprint('shape of X (test) :',X_test.shape)\n\n# %% [markdown] {\"id\":\"fFWozmaJ1Ej4\"}\n# ### <font color=\"green\">Scaling\n\n# %% [code] {\"id\":\"aXRLGb1U1Ej4\"}\nplt.plot(X_train[0]);\n\n# %% [markdown] {\"id\":\"KpZqKwrG1Ej4\"}\n# This graph shows that the every single feature has different value ranges. So we need to scale the data for better performances.\n\n# %% [code] {\"id\":\"9DbMBeIO1Ej5\"}\nscaler=StandardScaler()\n\n# %% [code] {\"id\":\"RPAL02za1Ej5\"}\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n\n# %% [code] {\"id\":\"hcbB5vV51Ej5\"}\nprint(\"Type of X_train :\",type(X_train))\nprint(\"Type of X_test :\",type(X_test))\n\n# %% [markdown] {\"id\":\"cTNM1W9I1Ej5\"}\n# As a result of using the **variance thershold** and **standard scaler** of *sklearn module* the pandas dataframe changed in numpy ndarray. So we are going to convert them into pandas dataframe.\n\n# %% [code] {\"id\":\"FUK0hkAQ1Ej5\"}\nX_train=pd.DataFrame(X_train,columns=cols)\nX_train.head()\n\n# %% [code] {\"id\":\"js2HBPML1Ej5\"}\nX_test=pd.DataFrame(X_test,columns=cols)\nX_test.head()\n\n# %% [code] {\"id\":\"cXyUpR6R1Ej5\"}\nprint('Type of X_train and X_test :',type(X_train),type(X_test))\n\n# %% [markdown] {\"id\":\"XuBGNoLS1Ej5\"}\n# ## Pipeline\n\n# %% [markdown] {\"id\":\"ELdRv2nj1Ej6\"}\n# As this is a regression problem we are going to use famous regression models -\n# \n#     Polynomial Regression\n#     RandomForest Regression\n#     Ridge Regression\n#     Lasso Regression\n#     Bayesian Ridge Regression\n\n# %% [code] {\"id\":\"QGAS-apP1Ej6\"}\nactr=[]\nacts=[]\nlstr=[]\nlsts=[]\n\n# %% [markdown] {\"id\":\"svdg1vLJ1Ej6\"}\n# ### Polynomial Regression\n\n# %% [code] {\"id\":\"LOCHL8WF1Ej6\"}\nloss=[]\nval_loss=[]\n\n# %% [code] {\"id\":\"01C5JejH1Ej6\"}\nfor i in range(2,5):\n  clf=make_pipeline(PolynomialFeatures(i),LinearRegression())\n  clf.fit(X_train,y_train)\n  y_pr=clf.predict(X_test)\n  y_x=clf.predict(X_train)\n  loss.append(mean_squared_error(y_train,y_x))\n  val_loss.append(mean_squared_error(y_test,y_pr))\n\n# %% [code] {\"id\":\"NQr4Q6CU1Ej6\"}\nplt.title('Model Loss')\nplt.xlabel('degree')\nplt.ylabel('MSE loss')\nplt.plot(range(2,5),loss/np.mean(loss),label='train loss')\nplt.plot(range(2,5),val_loss/np.mean(val_loss),label='validation loss')\nplt.legend()\nplt.show()\n\n# %% [markdown] {\"id\":\"S7zM7ruy1Ej6\"}\n# We can say that degree =3 is appropriate for the model as the validation has too little loss and after that it reaches to a very high peak. That means after degree>3 ,the model is overfitting.\n\n# %% [code] {\"id\":\"6nrknBUs1Ej6\"}\nprint('Train loss and validation loss of the polynomial function model :',loss[1],'and',val_loss[1])\n\n# %% [code] {\"id\":\"CRLO61dy1Ej7\"}\nclf=make_pipeline(PolynomialFeatures(3),LinearRegression())\nclf.fit(X_train,y_train)\nprint('train accuracy :',clf.score(X_train,y_train))\nprint('test accuracy :',clf.score(X_test,y_test))\n\n# %% [markdown] {\"id\":\"9qW-T1xW1Ej7\"}\n# Though degree=3 has the best accuracy over the polynomial function still it is very low and we can see that the validation accurcay is less than 50%. So, it is not a good model.\n\n# %% [code] {\"id\":\"5Erh3LmV1Ej7\"}\nactr.append(clf.score(X_train,y_train))\nacts.append(clf.score(X_test,y_test))\nlstr.append(loss[1])\nlsts.append(val_loss[1])\n\n# %% [markdown] {\"id\":\"rTHI0YfC1Ej7\"}\n# ### Random Forest Regression\n\n# %% [code] {\"id\":\"4uUUkOpl1Ej7\"}\nclf=RandomForestRegressor(random_state=0)\n\n# %% [code] {\"id\":\"Dyd-460W1Ej7\"}\nclf.fit(X_train,y_train)\ny_tr1=clf.predict(X_train)\ny_pr=clf.predict(X_test)\n\n# %% [code] {\"id\":\"54senaHR1Ej7\"}\nprint('train data accuracy :',clf.score(X_train,y_train))\nprint('test data accuracy :',clf.score(X_test,y_test))\nprint('loss of train data :',mean_squared_error(y_train,y_tr1))\nprint('loss of test data :',mean_squared_error(y_test,y_pr))\n\n# %% [markdown] {\"id\":\"Xbmdg3fq1Ej8\"}\n# So we can see the RFR really predicts the model very well and gives a quite accurate prediction.\n\n# %% [code] {\"id\":\"ac7e_jme1Ej8\"}\nactr.append(clf.score(X_train,y_train))\nacts.append(clf.score(X_test,y_test))\nlstr.append(mean_squared_error(y_train,y_tr1))\nlsts.append(mean_squared_error(y_test,y_pr))\n\n# %% [markdown] {\"id\":\"I9Vv3XhA1Ej8\"}\n# ### Ridge Regression\n\n# %% [code] {\"id\":\"EWmeKefd1Ej8\"}\nloss=[]\nval_loss=[]\n\n# %% [code] {\"id\":\"StkzqHYi1Ej8\"}\nfor i in range(1,11):\n  clf=Ridge(random_state=0,alpha=i/100.0)\n  clf.fit(X_train,y_train)\n  y_pr=clf.predict(X_test)\n  y_x=clf.predict(X_train)\n  loss.append(mean_squared_error(y_train,y_x))\n  val_loss.append(mean_squared_error(y_test,y_pr))\n\n# %% [code] {\"id\":\"EKkHGxqb1Ej8\"}\nplt.title('Model Loss')\nplt.xlabel('alpha')\nplt.ylabel('MSE loss')\nplt.plot(np.arange(1,11,1)/100,loss/np.mean(loss),label='train loss')\nplt.plot(np.arange(1,11,1)/100,val_loss/np.mean(val_loss),label='validation loss')\nplt.legend()\nplt.show()\n\n# %% [markdown] {\"id\":\"_ptdsNMF1Ej9\"}\n# So we can say that the increase in alpha also affecting the model badly and giving us more loss than before.\n\n# %% [code] {\"id\":\"t9eoDJwk1Ej9\"}\nclf=Ridge(random_state=0,alpha=0.01)\nclf.fit(X_train,y_train)\ny_tr1=clf.predict(X_train)\ny_pr=clf.predict(X_test)\n\nprint('train data accuracy :',clf.score(X_train,y_train))\nprint('test data accuracy :',clf.score(X_test,y_test))\nprint('loss of train data :',mean_squared_error(y_train,y_tr1))\nprint('loss of test data :',mean_squared_error(y_test,y_pr))\n\n# %% [markdown] {\"id\":\"jAd-bkP-1Ej9\"}\n# Still this model gives very bad stats in fitting.\n\n# %% [code] {\"id\":\"WUDsnPuG1Ej9\"}\nactr.append(clf.score(X_train,y_train))\nacts.append(clf.score(X_test,y_test))\nlstr.append(mean_squared_error(y_train,y_tr1))\nlsts.append(mean_squared_error(y_test,y_pr))\n\n# %% [markdown] {\"id\":\"HG1I2ReW1Ej9\"}\n# ### Lasso Regression\n\n# %% [code] {\"id\":\"A__HoLkk1Ej9\"}\nloss=[]\nval_loss=[]\n\n# %% [code] {\"id\":\"yFNHD6XR1Ej9\"}\nfor i in range(1,11):\n  clf=Lasso(random_state=0,alpha=i/100.0)\n  clf.fit(X_train,y_train)\n  y_pr=clf.predict(X_test)\n  y_x=clf.predict(X_train)\n  loss.append(mean_squared_error(y_train,y_x))\n  val_loss.append(mean_squared_error(y_test,y_pr))\n\n# %% [code] {\"id\":\"NabrSgza1Ej-\"}\nplt.title('Model Loss')\nplt.xlabel('alpha')\nplt.ylabel('MSE loss')\nplt.plot(np.arange(1,11,1)/100,loss/np.mean(loss),label='train loss')\nplt.plot(np.arange(1,11,1)/100,val_loss/np.mean(val_loss),label='validation loss')\nplt.legend()\nplt.show()\n\n# %% [markdown] {\"id\":\"3SWweeiI1Ej_\"}\n# Lasso model also gives us similar results as we got in Ridge. We're taking the alpha=0.01\n\n# %% [code] {\"id\":\"-VTnSTy91Ej_\"}\nclf=Lasso(random_state=0,alpha=0.01)\nclf.fit(X_train,y_train)\ny_tr1=clf.predict(X_train)\ny_pr=clf.predict(X_test)\n\nprint('train data accuracy :',clf.score(X_train,y_train))\nprint('test data accuracy :',clf.score(X_test,y_test))\nprint('loss of train data :',mean_squared_error(y_train,y_tr1))\nprint('loss of test data :',mean_squared_error(y_test,y_pr))\n\n# %% [code] {\"id\":\"0U2nWKXw1Ej_\"}\nactr.append(clf.score(X_train,y_train))\nacts.append(clf.score(X_test,y_test))\nlstr.append(mean_squared_error(y_train,y_tr1))\nlsts.append(mean_squared_error(y_test,y_pr))\n\n# %% [markdown] {\"id\":\"8yiW_fVy1Ej_\"}\n# ## Bayesian Ridge Regression\n\n# %% [code] {\"id\":\"_C0T1qgy1Ej_\"}\nclf=BayesianRidge()\n\n# %% [code] {\"id\":\"bsxh1a2x1EkA\"}\nclf.fit(X_train,y_train)\ny_tr1=clf.predict(X_train)\ny_pr=clf.predict(X_test)\n\nprint('train data accuracy :',clf.score(X_train,y_train))\nprint('test data accuracy :',clf.score(X_test,y_test))\nprint('loss of train data :',mean_squared_error(y_train,y_tr1))\nprint('loss of test data :',mean_squared_error(y_test,y_pr))\n\n# %% [code] {\"id\":\"7uluzLld1EkA\"}\nactr.append(clf.score(X_train,y_train))\nacts.append(clf.score(X_test,y_test))\nlstr.append(mean_squared_error(y_train,y_tr1))\nlsts.append(mean_squared_error(y_test,y_pr))\n\n# %% [markdown] {\"id\":\"NP2KYlh21EkA\"}\n# ## Model Evaluation\n\n# %% [code] {\"id\":\"6MaJZd0m1EkA\"}\nmodels=['Polynomial','Random Forest','Ridge','Lasso','Bayesian Ridge']\n\n# %% [code] {\"id\":\"l4NOd86T1EkA\"}\nplt.title('Model Accuracy')\nplt.plot(models,actr,label='train data')\nplt.plot(models,acts,label='validation data')\nplt.legend()\nplt.show()\n\nplt.title('Model Loss')\nplt.plot(models,lstr,label='train data')\nplt.plot(models,lsts,label='validation data')\nplt.legend()\nplt.show()\n\n# %% [markdown] {\"id\":\"RrV2DTPz1EkA\"}\n# **Model performance : Random Forest > Ridge > Bayesian Ridge > Lasso > Polynomial**\n#                        \n# After model evaluation we can conclude that, high dimensional data can not be fit well in low dimensional models and can give abrupt conclusions that may led to lower accuracy.\n# \n# The RFR uses leafs that can reduce the dimensional complexity and generalize the model in a better approach. Thus it presents the best accuracy over the data.\n\n# %% [markdown] {\"id\":\"-d6a5tOX1EkA\"}\n# # 70:30 split\n\n# %% [code] {\"id\":\"4TcHLdG41EkA\"}\nfrom sklearn.model_selection import train_test_split\n\ntrainflights, testflights, ytrain, ytest = train_test_split(data, y, train_size=0.7,test_size=0.3, random_state=0)\n\n# %% [code] {\"id\":\"-JrijFH-1EkB\"}\ns = (trainflights.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nn = (trainflights.dtypes == ('float64','int64'))\nnumerical_cols = list(n[n].index)\n\n# %% [code] {\"id\":\"KhPpfG741EkB\",\"outputId\":\"d6de980d-23d4-47e7-ef1f-1a2471185a8b\"}\n#checking the columns containing categorical columns:\nprint(object_cols)\n\n# %% [code] {\"id\":\"8BKUR03C1EkB\"}\n#using One Hot Encoder to make the categorical columns usable\n\noneHot = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\noneHottrain = pd.DataFrame(oneHot.fit_transform(trainflights[object_cols]))\noneHottest = pd.DataFrame(oneHot.transform(testflights[object_cols]))\n\n#reattaching index since OneHotEncoder removes them:\noneHottrain.index = trainflights.index\noneHottest.index = testflights.index \n\n#dropping the old categorical columns:\ncattraincol = trainflights.drop(object_cols, axis=1)\ncattestcol = testflights.drop(object_cols, axis=1)\n\n#concatenating the new columns:\ntrainflights = pd.concat([cattraincol, oneHottrain], axis=1)\ntestflights = pd.concat([cattestcol, oneHottest], axis=1)\n\n# %% [code] {\"id\":\"unG9m3ke1EkB\",\"outputId\":\"22f70afd-4c64-437b-b54d-7cc3895b8508\"}\n#scaling the values\n\ntrainf = trainflights.values\ntestf = testflights.values\n\nminmax = MinMaxScaler()\n\ntrainflights = minmax.fit_transform(trainf)\ntestflights = minmax.transform(testf)\n\n#defining a way to find Mean Absolute Percentage Error:\ndef PercentError(preds, ytest):\n  error = abs(preds - ytest)\n\n  errorp = np.mean(100 - 100*(error/ytest))\n\n  print('the accuracy is:', errorp)\n\n# %% [code] {\"id\":\"Drogdln11EkB\"}\n#implementing the algo:\nmodel = RandomForestRegressor(n_estimators=100, random_state=0, verbose=1)\n\n#fitting the data to random forest regressor:\nmodel.fit(trainflights, ytrain)\n\n# %% [code] {\"id\":\"keEE779w1EkB\"}\n#predicting the test dataset:\npreds = model.predict(testflights)\nPercentError(preds, ytest)\n\n# %% [code] {\"id\":\"n6bheUSa1EkB\"}\n#using linear regression:\nLinearModel = LinearRegression()\nLinearModel.fit(trainflights, ytrain)\n\n# %% [code] {\"id\":\"7v5CthiL1EkC\"}\n#predicting on the test dataset:\nLinearPredictions = LinearModel.predict(testflights)\nPercentError(LinearPredictions, ytest)\n\n# %% [markdown] {\"id\":\"wmOfthNw1EkC\"}\n# # Decision-Tree\n\n# %% [markdown] {\"id\":\"X86SWxtj1EkC\"}\n# ## Prediction for Small bags & The average price\n\n# %% [code] {\"id\":\"S3LacoKR1EkC\"}\ndf=data[[\"year\",\"Small Bags\",\"Large Bags\", \"AveragePrice\"]]\n\ndf = df.sample(n=50,replace=True)\n#df=df.head(50)\ndf.tail()\n\n# %% [code] {\"id\":\"Y54mhavt1EkF\"}\ny=df.iloc[:,1].values\nx=df.iloc[:,-1].values\n\n# %% [code] {\"id\":\"m28ww4tm1EkG\"}\nx=x.reshape(len(x),1)\ny=y.reshape(len(y),1)\n\n# %% [markdown] {\"id\":\"S9nv9VkZ1EkG\"}\n# ### Training the Decision Tree Regression model on the whole dataset\n\n# %% [code] {\"id\":\"MIaU-C9a1EkG\"}\nfrom sklearn.tree import DecisionTreeRegressor\nregressor=DecisionTreeRegressor(random_state=0)\nregressor.fit(x,y)\n\n# %% [code] {\"id\":\"3OzlCotZ1EkG\"}\nDecisionTreeRegressor(random_state=0)\n\n# %% [code] {\"id\":\"FaRGTvzj1EkG\"}\n## Predicting a new result: What is the price for the \nregressor.predict([[8042.21]])\n\n# %% [code] {\"id\":\"lpXWKZ3v1EkG\"}\nregressor.predict([[8000]])\n\n# %% [code] {\"id\":\"13XBCzpY1EkG\"}\nregressor.predict([[18000]])\n\n# %% [markdown] {\"id\":\"hXzFcwEV1EkG\"}\n# ### Visualization\n\n# %% [code] {\"id\":\"rlk3yMrU1EkG\"}\nX_grid = np.arange(min(x), max(x), 0.001)\nX_grid = X_grid.reshape((len(X_grid), 1))\n\nplt.scatter(x,y, color=\"red\")\nplt.plot(X_grid, regressor.predict(X_grid), color=\"blue\")\n\nplt.title(\"Decision Tree Regression for Avocado Prices\")\nplt.xlabel(\"Avocado Small Bags\")\nplt.ylabel(\"Price\")\nplt.show()\n\n# %% [markdown] {\"id\":\"Y22RDjpA1EkH\"}\n# # Findings\n\n# %% [markdown] {\"id\":\"GAxS_2Aw1EkH\"}\n# - Conventional avocados sell way more than organic avocados and cost less. Therefore, Total volume, along with other volume variables, and average price, will work well to predict our target variable, type, in our classification model\n# \n# - Average price and total volume move in different directions, this will come in handy when doing a regression analysis over our target variable, which is average price\n# \n# - In the time series exploration, we see that there is a pike in total volume and a drop in prices at the beggining of the month, hinting for seasonality and forecasting possibilities\n\n# %% [markdown] {\"id\":\"50Weh7RH1EkH\"}\n# Normally, there is an inverse relationship between supply and prices. When there is an overproduction of avocados they will have a negative impact on the market price of avocados. Let's see if this is the case for both conventional and organic avocados.\n# Conventional: At the end of 2017 we can see a large drop in prices, at the same time there is an increasing amount of volume of avocados in the market.\n# Organic: Same happens with organic avocados, at the end of 2017 there is a big drop and we can see a huge increase in volume.\n# Volume peaks: Notice how each volume peak is a signal for an upcoming drop in avocado prices.\n\n# %% [code] {\"id\":\"lFDMgBLr1EkH\"}\n","metadata":{"_uuid":"0f7b0189-d817-4d20-8383-f1d7263b7829","_cell_guid":"d75898b9-3f12-43e3-9709-3923fab44441","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}